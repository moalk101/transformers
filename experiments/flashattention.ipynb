{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "X_hDThfYVY4-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self,vocab_size,d_model):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=d_model)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.embedding(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "kgphPG7GL3iB"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len,d_model)\n",
        "        positions = torch.arange(0,max_len).unsqueeze(1)\n",
        "\n",
        "        steps = torch.arange(0,d_model,2)\n",
        "\n",
        "        pe[:,0::2] = torch.sin(positions / (10000) ** (steps / d_model))\n",
        "        pe[:,1::2] = torch.cos(positions / (10000) ** (steps / d_model))\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer(\"pe\",pe)\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        length = x.size(1)\n",
        "\n",
        "        x = x + self.pe[:,:length,:]\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2me2k3DL_63"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from flash_attn import flash_attn_func\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, mask_future=False):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.future_mask = mask_future\n",
        "        self.d_k = d_model // n_heads\n",
        "        assert self.d_k % 8 == 0\n",
        "\n",
        "        # Linear projections\n",
        "        self.query_transform = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.key_transform   = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.value_transform = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.output_transform = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "\n",
        "        B, S, _ = x.size()\n",
        "        x = x.view(B, S, self.n_heads, self.d_k).permute(0, 2, 1, 3).contiguous()\n",
        "        return x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        B, H, S, D = x.size()\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        return x.view(B, S, self.d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        Q = self.query_transform(q)  \n",
        "        K = self.key_transform(k)\n",
        "        V = self.value_transform(v)\n",
        "\n",
        "        B, S_q, _ = Q.size()\n",
        "        _, S_k, _ = K.size()\n",
        "\n",
        "        Q = Q.view(B, S_q, self.n_heads, self.d_k)\n",
        "        K = K.view(B, S_k, self.n_heads, self.d_k)\n",
        "        V = V.view(B, S_k, self.n_heads, self.d_k)\n",
        "        out = flash_attn_func(\n",
        "            Q, K, V,\n",
        "            dropout_p=0.0,\n",
        "            causal=self.future_mask\n",
        "        )\n",
        "\n",
        "        out = out.reshape(B, S_q, self.d_model)\n",
        "        out = self.output_transform(out)\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "dG7B7AmWMEAn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class PostionalFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model,d_hl,dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_hl),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_hl, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.ff(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Gls_R1mCMGIt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BaseTransformerLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,input_dim, num_heads, feature_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(input_dim,num_heads)\n",
        "        self.feature_transformation = PostionalFeedForward(input_dim,feature_dim,dropout)\n",
        "\n",
        "        self.layer_norm_1 = nn.LayerNorm(input_dim)\n",
        "        self.layer_norm_2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask = None):\n",
        "        attn_out = self.self_attention(x,x,x,mask)\n",
        "        x = x + self.dropout1(attn_out)\n",
        "        x = self.layer_norm_1(x)\n",
        "\n",
        "        ff_out = self.feature_transformation(x)\n",
        "        x = x +self.dropout2(ff_out)\n",
        "        x = self.layer_norm_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self,input_dim,num_heads, feature_dim,dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attention = MultiHeadAttention(\n",
        "            input_dim,\n",
        "            num_heads,\n",
        "            mask_future=True\n",
        "        )\n",
        "\n",
        "        self.encoder_attention = MultiHeadAttention(input_dim,num_heads,mask_future=False)\n",
        "        self.feature_transformation = PostionalFeedForward(\n",
        "            input_dim,feature_dim,dropout\n",
        "        )\n",
        "        self.layer_norm_1 = nn.LayerNorm(input_dim)\n",
        "        self.layer_norm_2 = nn.LayerNorm(input_dim)\n",
        "        self.layer_norm_3 = nn.LayerNorm(input_dim)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, target, encoder_output, src_mask=None, target_mask=None):\n",
        "\n",
        "        attn_out = self.self_attention(target,target,target,mask=target_mask)\n",
        "        target = target + self.dropout1(attn_out)\n",
        "        target = self.layer_norm_1(target)\n",
        "\n",
        "        attn_out = self.encoder_attention(q=target,k=encoder_output,v=encoder_output,mask=src_mask)\n",
        "        target = target + self.dropout1(attn_out)\n",
        "        target = self.layer_norm_2(target)\n",
        "\n",
        "        ff_out = self.feature_transformation(target)\n",
        "        target = target + self.dropout1(ff_out)\n",
        "        target = self.layer_norm_3(target)\n",
        "        return target\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "FAT_HE4MMRPS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_model,  n_heads, dim_feedforward, dropout, num_layers\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [\n",
        "                TransformerDecoderLayer(d_model, n_heads, dim_feedforward, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, tgt_emb, src_emb, src_mask, tgt_mask):\n",
        "        for layer in self.decoder_layers:\n",
        "            tgt_emb = layer(tgt_emb, src_emb, src_mask, tgt_mask)\n",
        "        return tgt_emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "Xu6Lf5EBMURK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_model,  n_heads, dim_feedforward, dropout, num_layers\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [\n",
        "                BaseTransformerLayer(d_model, n_heads, dim_feedforward, dropout)\n",
        "                for _ in range(num_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, src_emb, src_mask):\n",
        "        for layer in self.encoder_layers:\n",
        "            src_emb = layer(src_emb, src_mask)\n",
        "        return src_emb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "at4Y6iUUMW3x"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TransformerEmbedding(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, d_model, max_len):\n",
        "        super().__init__()\n",
        "\n",
        "        self.token_emb = Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "    def forward(self, token):\n",
        "        device = token.device\n",
        "        token_emb = self.token_emb(token).to(device,non_blocking=True)\n",
        "        pos_emb = self.pos_emb(token_emb).to(device,non_blocking=True)\n",
        "\n",
        "        return pos_emb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "5notdlpgMaGx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        num_encoder_layers: int,\n",
        "        num_decoder_layers: int,\n",
        "        dim_feedforward: int,\n",
        "        dropout: float,\n",
        "        max_len: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = TransformerEmbedding(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            max_len=max_len,\n",
        "        )\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            num_layers=num_encoder_layers,\n",
        "            d_model=d_model,\n",
        "            n_heads=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            num_layers=num_decoder_layers,\n",
        "            d_model=d_model,\n",
        "            n_heads=n_heads,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        self.output_projection.weight = self.embedding.token_emb.embedding.weight\n",
        "\n",
        "    def generate(self, src, max_length=50):\n",
        "        device = next(self.parameters()).device\n",
        "        src = src.to(device)\n",
        "        PAD = 0\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        all_outputs = []\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            seq = src[i]\n",
        "            length = (seq != PAD).sum().item()\n",
        "            trimmed_seq = seq[:length].unsqueeze(0)\n",
        "\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                # Encoder\n",
        "                src_emb = self.embedding(trimmed_seq)\n",
        "                memory = self.encoder(src_emb, None)\n",
        "\n",
        "                # Start token for decoder\n",
        "                tgt = torch.tensor([[1]], device=device, dtype=torch.long)\n",
        "\n",
        "                # Autoregressive decoding loop\n",
        "                for _ in range(max_length):\n",
        "                    tgt_emb = self.embedding(tgt)\n",
        "                    output = self.decoder(tgt_emb, memory, None, None)\n",
        "                    logits = self.output_projection(output)\n",
        "                    next_token = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "                    tgt = torch.cat([tgt, next_token.unsqueeze(1)], dim=1)\n",
        "\n",
        "                    if next_token.item() == 2:  # EOS token\n",
        "                        break\n",
        "\n",
        "            all_outputs.append(tgt.squeeze(0))\n",
        "\n",
        "        return all_outputs\n",
        "\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.Tensor,\n",
        "        tgt: torch.Tensor,\n",
        "        src_mask: torch.Tensor = None,\n",
        "        tgt_mask: torch.Tensor = None\n",
        "    ):\n",
        "\n",
        "        src_emb = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "\n",
        "        encoder_output = self.encoder(\n",
        "            src_emb,\n",
        "            src_mask=src_mask,\n",
        "        )\n",
        "\n",
        "        masked_encoder_output = encoder_output * src_mask.unsqueeze(-1)\n",
        "\n",
        "        output = self.decoder(\n",
        "            tgt_emb,\n",
        "            masked_encoder_output,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_mask=src_mask\n",
        "\n",
        "        )\n",
        "\n",
        "        logits = self.output_projection(output)\n",
        "\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "CDZagxikMfYC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class TranslationDataSet:\n",
        "\n",
        "    def __init__(self,min_len=5,max_len=64,max_ratio=1.5,limit=None):\n",
        "        self.WHITELIST = \"abcdefghijklmnopqrstuvwxyzäöüß0123456789.,!?()[]{}:;-&$@#%£€/\\\\|_+*¥ \"\n",
        "        self.WHITELIST_SET = set(self.WHITELIST.lower())\n",
        "        self.min_ratio = 1 / max_ratio\n",
        "        self.max_ratio = max_ratio\n",
        "        self.max_len = max_len\n",
        "        self.min_len = min_len\n",
        "        self.data = load_dataset(\"wmt17\", \"de-en\")\n",
        "        self.limit = limit if limit != None else len(self.data[\"train\"])\n",
        "\n",
        "    def get_wmt17_datset(self,split=\"train\"):\n",
        "        return self.data[split].select(range(self.limit))\n",
        "\n",
        "    def _preprocess_text(self,text):\n",
        "        text = text.lower()\n",
        "\n",
        "        text = re.sub(r'http\\S+|www\\S+|<.*?>', '', text)\n",
        "\n",
        "        text = \"\".join(c for c in text if c in self.WHITELIST_SET)\n",
        "\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def clean_sentence_pair(self,example: dict):\n",
        "        source_lang = 'de'\n",
        "        target_lang = 'en'\n",
        "\n",
        "        source_text = example['translation'][source_lang]\n",
        "        target_text = example['translation'][target_lang]\n",
        "\n",
        "        keep_example = True\n",
        "        cleaned_source = self._preprocess_text(source_text)\n",
        "        cleaned_target = self._preprocess_text(target_text)\n",
        "\n",
        "\n",
        "        source_len = len(cleaned_source.split())\n",
        "        target_len = len(cleaned_target.split())\n",
        "\n",
        "\n",
        "        if not (self.min_len <= source_len <= self.max_len and self.min_len <= target_len <= self.max_len):\n",
        "            keep_example = False\n",
        "\n",
        "        if source_len > 0 and target_len > 0:\n",
        "            ratio = source_len / target_len\n",
        "            if not (self.min_ratio <= ratio <= self.max_ratio):\n",
        "                keep_example = False\n",
        "        else:\n",
        "            keep_example = False\n",
        "\n",
        "        example['translation'][source_lang] = cleaned_source\n",
        "        example['translation'][target_lang] = cleaned_target\n",
        "        example['keep'] = keep_example\n",
        "\n",
        "        return example\n",
        "\n",
        "\n",
        "class TranslationTorchDataset(Dataset):\n",
        "    def __init__(self,dataset, tokenizer,max_len=64, src_lang=\"de\",tgt_lang=\"en\"):\n",
        "        super().__init__()\n",
        "        self.data = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        element = self.data[index][\"translation\"]\n",
        "\n",
        "        src_ids = self.tokenizer.encode(element[self.src_lang])[:self.max_len]\n",
        "        tgt_ids = self.tokenizer.encode(element[self.tgt_lang])[:self.max_len]\n",
        "\n",
        "        return {\"src\": torch.tensor(src_ids,dtype=torch.long),\"tgt\": torch.tensor(tgt_ids,dtype=torch.long)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "h4LGQTWLMjeo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.optim.lr_scheduler import LRScheduler\n",
        "\n",
        "class TransformerLR(LRScheduler):\n",
        "\n",
        "    def __init__(self, optimizer,d_model,warmup_steps, last_epoch = -1):\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.last_epoch = last_epoch\n",
        "        super().__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        step = max(1,self.last_epoch + 1)\n",
        "        lr = self.d_model ** -0.5 * min(step ** -0.5, step * self.warmup_steps ** -1.5)\n",
        "        lrs = [lr for _ in self.optimizer.param_groups]\n",
        "\n",
        "        return lrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "2DB5Yed6Mmpu"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from transformers import GPT2TokenizerFast\n",
        "import os\n",
        "\n",
        "class MyTokenizer:\n",
        "    def __init__(self, vocab_size=50000, save_dir=\"my_gpt2_bpe\"):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.save_dir = save_dir\n",
        "        self.tokenizer = None\n",
        "        self.pad_token_id = None\n",
        "        self.bos_token_id = None\n",
        "        self.eos_token_id = None\n",
        "\n",
        "    def train(self, corpus):\n",
        "        tokenizer = Tokenizer(BPE(unk_token=\"<|unk|>\"))\n",
        "        tokenizer.pre_tokenizer = ByteLevel(add_prefix_space=True)\n",
        "        tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "        special_tokens = [\n",
        "            \"[PAD]\",\n",
        "            \"[BOS]\",\n",
        "            \"[EOS]\",\n",
        "            \"[UNK]\",\n",
        "        ]\n",
        "\n",
        "        trainer = BpeTrainer(\n",
        "            vocab_size=self.vocab_size,\n",
        "            special_tokens=special_tokens,\n",
        "        )\n",
        "\n",
        "        tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
        "\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        tokenizer.save(f\"{self.save_dir}/tokenizer.json\")\n",
        "\n",
        "        self.tokenizer = GPT2TokenizerFast(\n",
        "            tokenizer_file=f\"{self.save_dir}/tokenizer.json\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            bos_token=\"[BOS]\",\n",
        "            eos_token=\"[EOS]\",\n",
        "            unk_token=\"[UNK]\",\n",
        "        )\n",
        "        self.pad_token_id = self.tokenizer.pad_token_id\n",
        "        self.bos_token_id = self.tokenizer.bos_token_id\n",
        "        self.eos_token_id = self.tokenizer.eos_token_id\n",
        "\n",
        "    def load(self, path):\n",
        "        self.tokenizer = GPT2TokenizerFast(\n",
        "            tokenizer_file=f\"{path}/tokenizer.json\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            bos_token=\"[BOS]\",\n",
        "            eos_token=\"[EOS]\",\n",
        "            unk_token=\"[UNK]\",\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    def encode(self, text,add_special_tokens=True):\n",
        "        ids = self.tokenizer.encode(text)\n",
        "        if add_special_tokens:\n",
        "            ids = [self.tokenizer.bos_token_id] + ids + [self.tokenizer.eos_token_id]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self.tokenizer.decode(ids)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "_BlTuAHYT18E"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import sacrebleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def corpus_iterator(dictionary):\n",
        "    for ex in dictionary:\n",
        "        yield ex[\"translation\"][\"de\"]\n",
        "        yield ex[\"translation\"][\"en\"]\n",
        "\n",
        "\n",
        "def collate_fn(batch, pad_id):\n",
        "    srcs = [b[\"src\"] for b in batch]\n",
        "    tgts = [b[\"tgt\"] for b in batch]\n",
        "\n",
        "    src_batch = nn.utils.rnn.pad_sequence(srcs, batch_first=True, padding_value=pad_id)\n",
        "    tgt_batch = nn.utils.rnn.pad_sequence(tgts, batch_first=True, padding_value=pad_id)\n",
        "\n",
        "    src_mask = (src_batch != pad_id)\n",
        "    tgt_padding_mask = (tgt_batch != pad_id)\n",
        "\n",
        "    return src_batch, tgt_batch, src_mask.long(), tgt_padding_mask.long()\n",
        "\n",
        "class CollateWithPad:\n",
        "    def __init__(self, pad_id):\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return collate_fn(batch, self.pad_id)\n",
        "\n",
        "def compute_lengths(dataset):\n",
        "    return [len(item[\"src\"]) + len(item[\"tgt\"]) for item in dataset]\n",
        "\n",
        "def run_validation(model, val_loader, loss_fn, device,limit=-1):\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src, tgt, src_mask, tgt_padding_mask) in enumerate(val_loader):\n",
        "            src = src.to(device, non_blocking=True)\n",
        "            tgt = tgt.to(device, non_blocking=True)\n",
        "            src_mask = src_mask.to(device, non_blocking=True)\n",
        "            tgt_padding_mask = tgt_padding_mask.to(device, non_blocking=True)\n",
        "\n",
        "            decoder_input = tgt[:, :-1]\n",
        "            decoder_target = tgt[:, 1:]\n",
        "            tgt_mask_input = tgt_padding_mask[:, :-1]\n",
        "\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                logits = model(\n",
        "                    src,\n",
        "                    decoder_input,\n",
        "                    src_mask=src_mask,\n",
        "                    tgt_mask=tgt_mask_input\n",
        "                )\n",
        "                loss = loss_fn(\n",
        "                    logits.reshape(-1, logits.size(-1)),\n",
        "                    decoder_target.reshape(-1)\n",
        "                )\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            if batch_idx == limit:\n",
        "                break\n",
        "\n",
        "    model.train()\n",
        "    return total_val_loss / num_batches\n",
        "\n",
        "def compute_bleu(\n",
        "    model,\n",
        "    val_loader,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    max_length=50,\n",
        "    max_batches=None\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    hypotheses = []\n",
        "    references = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (src, tgt, src_mask, _) in enumerate(\n",
        "            tqdm(val_loader, desc=\"Computing BLEU\", leave=False)\n",
        "        ):\n",
        "            if max_batches is not None and i >= max_batches:\n",
        "                break\n",
        "\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            pred = model.generate(\n",
        "                src,\n",
        "                max_length=max_length\n",
        "            )\n",
        "\n",
        "            for p, r in zip(pred, tgt):\n",
        "                p = [\n",
        "                    t.item() for t in p\n",
        "                    if t.item() not in {\n",
        "                        0,\n",
        "                        1,\n",
        "                        2\n",
        "                    }\n",
        "                ]\n",
        "                r = [\n",
        "                    t.item() for t in r\n",
        "                    if t.item() not in {\n",
        "                        0,\n",
        "                        1,\n",
        "                        2\n",
        "                    }\n",
        "                ]\n",
        "\n",
        "                hypotheses.append(tokenizer.decode(p))\n",
        "                references.append(tokenizer.decode(r))\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, [references])\n",
        "    return bleu.score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-IB5ZU_u_nA",
        "outputId": "04fc044f-1231-4916-972b-28c02b60c36e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading existing tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-488129881.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "Epoch 1/5:   0%|          | 0/157 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 finished. Loss: 92.7942 | Avg batch time: 0.167s | Avg peak memory: 48759.4 MBValidation loss: 32.86406222256747\n",
            "validation loss got improved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 finished. Loss: 31.9296 | Avg batch time: 0.168s | Avg peak memory: 39342.8 MBValidation loss: 26.42453384399414\n",
            "validation loss got improved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 finished. Loss: 26.9332 | Avg batch time: 0.168s | Avg peak memory: 31951.4 MBValidation loss: 25.149976210160688\n",
            "validation loss got improved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 finished. Loss: 22.7849 | Avg batch time: 0.168s | Avg peak memory: 31951.4 MBValidation loss: 17.41531077298251\n",
            "validation loss got improved!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 finished. Loss: 18.3375 | Avg batch time: 0.168s | Avg peak memory: 31951.4 MBValidation loss: 14.375019853765314\n",
            "validation loss got improved!\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def main():\n",
        "    ds = TranslationDataSet(limit=150000)\n",
        "    raw = ds.get_wmt17_datset()\n",
        "    cleaned = raw.map(ds.clean_sentence_pair)\n",
        "    cleaned = cleaned.filter(lambda x: x[\"keep\"])\n",
        "\n",
        "    tokenizer_path = r\"/content/drive/MyDrive/mytransformer/models/WMTBPETokenizer\"\n",
        "\n",
        "    import os\n",
        "    if os.path.exists(tokenizer_path):\n",
        "        print(\"Loading existing tokenizer...\")\n",
        "        tokenizer = MyTokenizer().load(tokenizer_path)\n",
        "    else:\n",
        "        print(\"Tokenizer not found. Training a new tokenizer...\")\n",
        "        tokenizer = MyTokenizer()\n",
        "        iterator = corpus_iterator(cleaned)  # or whatever iterator you have\n",
        "        tokenizer.train(iterator)\n",
        "        tokenizer.save(tokenizer_path)\n",
        "        print(f\"Tokenizer saved at {tokenizer_path}\")\n",
        "\n",
        "    pad_id = tokenizer.tokenizer.pad_token_id\n",
        "    torchdataset = TranslationTorchDataset(cleaned, tokenizer)\n",
        "\n",
        "    subset_size = 100000\n",
        "    subset_dataset = torch.utils.data.Subset(torchdataset, list(range(subset_size)))\n",
        "    train_size, val_size = 80000, 20000\n",
        "    train_dataset, val_dataset = random_split(subset_dataset, [train_size, val_size])\n",
        "    collator = CollateWithPad(pad_id)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=False, collate_fn=collator)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, collate_fn=collator)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = Transformer(50000,512,8,6,6,2048,0.1,64).to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-9)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    scheduler = TransformerLR(optimizer, d_model=model.d_model, warmup_steps=4000)\n",
        "\n",
        "    num_epochs = 5\n",
        "    writer = SummaryWriter(log_dir=\"/content/drive/MyDrive/mytransformer/models/Transformermodel/flashattention/log\")\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_globsl_val_loss = 999999\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        batch_times = []\n",
        "        batch_memories = []\n",
        "\n",
        "        for batch_idx, (src, tgt, src_mask, tgt_padding_mask) in enumerate(\n",
        "              tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "          ):\n",
        "            start_time = time.perf_counter()  \n",
        "            optimizer.zero_grad()\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            src_mask, tgt_padding_mask = src_mask.to(device), tgt_padding_mask.to(device)\n",
        "\n",
        "            decoder_input = tgt[:, :-1]\n",
        "            decoder_target = tgt[:, 1:]\n",
        "            tgt_mask_input = tgt_padding_mask[:, :-1]\n",
        "\n",
        "            torch.cuda.reset_peak_memory_stats(device) \n",
        "\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                logits = model(\n",
        "                    src,\n",
        "                    decoder_input,\n",
        "                    src_mask=src_mask,\n",
        "                    tgt_mask=tgt_mask_input\n",
        "                )\n",
        "                loss = loss_fn(\n",
        "                    logits.reshape(-1, logits.size(-1)),\n",
        "                    decoder_target.reshape(-1)\n",
        "                )\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            \n",
        "            batch_time = time.perf_counter() - start_time\n",
        "            batch_times.append(batch_time)\n",
        "            writer.add_scalar(\"Time/batch\", batch_time, epoch * len(train_loader) + batch_idx)\n",
        "\n",
        "            \n",
        "            batch_memory = torch.cuda.max_memory_allocated(device) / 1024 ** 2  # MB\n",
        "            batch_memories.append(batch_memory)\n",
        "\n",
        "            writer.add_scalar(\"Memory/peak_batch\", batch_memory, epoch * len(train_loader) + batch_idx)\n",
        "        globsl_val_loss = run_validation(model, val_loader, loss_fn, device,limit=10)\n",
        "        # bleu = compute_bleu(model, val_loader, tokenizer, device=\"cuda\", max_batches=1)\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        avg_batch_time = sum(batch_times) / len(batch_times)\n",
        "        avg_batch_memory = sum(batch_memories) / len(batch_memories)\n",
        "        print(f\"Epoch {epoch+1} finished. Loss: {avg_train_loss:.4f} | \"\n",
        "              f\"Avg batch time: {avg_batch_time:.3f}s | \"\n",
        "              f\"Avg peak memory: {avg_batch_memory:.1f} MB\"\n",
        "              f\"Validation loss: {globsl_val_loss}\")\n",
        "\n",
        "        writer.add_scalar(\"Loss/train_epoch\", avg_train_loss, epoch)\n",
        "        # writer.add_scalar(\"BLEU/epoch\", bleu, epoch)\n",
        "        writer.add_scalar(\"Loss/validation_epoch\", globsl_val_loss, epoch)\n",
        "        if globsl_val_loss < best_globsl_val_loss:\n",
        "            print(\"validation loss got improved!\")\n",
        "            checkpoint = {\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"loss\": avg_train_loss,\n",
        "                \"global_step\" : epoch\n",
        "            }\n",
        "            best_globsl_val_loss = globsl_val_loss\n",
        "            torch.save(checkpoint, \"/content/drive/MyDrive/mytransformer/models/Transformermodel/flashattention/checkpoint_best_val_loss.pt\")\n",
        "    writer.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    torch.multiprocessing.freeze_support()\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
